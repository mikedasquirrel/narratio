# Pipeline Caching Guide

## Overview

The narrative optimization framework now includes **automatic joblib caching** for all pipeline operations. This dramatically improves performance by memoizing expensive transformations and avoiding redundant computation.

**Key Benefits:**
- ‚ö° **10-100x speedup** for repeated runs with same data
- üîí **Automatic invalidation** when code or parameters change
- üéØ **Zero configuration** - enabled by default
- üìä **Cache management** tools for inspection and cleanup

## Quick Start

### Basic Usage (Caching Enabled by Default)

```python
from narrative_optimization.src.pipelines import NarrativePipeline
from narrative_optimization.src.transformers import LinguisticPatternsTransformer

# Create pipeline (caching enabled by default)
pipeline = NarrativePipeline(
    'my_narrative',
    hypothesis='Linguistic patterns predict outcomes'
)

pipeline.add_step('linguistic', LinguisticPatternsTransformer(), 'Extract patterns')
pipe = pipeline.build()

# First run: computes and caches results
X_transformed = pipe.fit_transform(X_train, y_train)

# Second run: instant (uses cache)
X_transformed = pipe.transform(X_train)
```

### Verbose Mode (See Cache Activity)

```python
pipeline = NarrativePipeline(
    'my_narrative',
    use_cache=True,
    cache_verbose=1  # Show cache hits/misses
)
```

Output:
```
[Memory] Calling sklearn.pipeline._fit_transform_one...
[Memory]...done.
```

### Disable Caching

```python
# For one-off experiments or testing
pipeline = NarrativePipeline(
    'experiment',
    use_cache=False
)
```

## How It Works

### What Gets Cached?

Joblib caches the results of:
1. **Transformer fit operations** - Model training
2. **Transformer transform operations** - Feature extraction
3. **Intermediate pipeline steps** - Each step independently

### Cache Keys

Cache keys are automatically generated from:
- Transformer class and code
- Transformer parameters
- Input data characteristics (shape, sample)
- Python version and dependencies

### Automatic Invalidation

The cache is **automatically invalidated** when:
- ‚úÖ Transformer code changes
- ‚úÖ Transformer parameters change
- ‚úÖ Input data changes (shape or content)
- ‚úÖ Python version changes
- ‚úÖ scikit-learn version changes

This ensures you **never get stale results**.

## Cache Management

### Inspect Cache Status

```python
from narrative_optimization.src.pipelines import get_cache_manager

manager = get_cache_manager()

# Get comprehensive statistics
stats = manager.get_cache_stats()
print(f"Total cache size: {stats['total_size_mb']:.1f} MB")
print(f"Pipelines cached: {stats['pipeline_count']}")

# Print formatted report
manager.print_cache_report()
```

Example output:
```
======================================================================
NARRATIVE PIPELINE CACHE REPORT
======================================================================

Cache Location: /Users/you/.narrative_cache
Total Size: 245.3 MB
Pipeline Caches: 5
Oldest Cache: 2025-11-01 14:30
Newest Cache: 2025-11-10 09:15

----------------------------------------------------------------------
CACHED PIPELINES (sorted by size):
----------------------------------------------------------------------
Pipeline Name                            Size (MB)    Last Modified
----------------------------------------------------------------------
imdb_narrative_full                         125.40  2025-11-10 09:15
crypto_ensemble                              67.20  2025-11-09 16:42
mental_health_nominative                     32.50  2025-11-08 11:20
startup_credibility                          15.10  2025-11-05 08:30
test_pipeline                                 5.10  2025-11-01 14:30
======================================================================
```

### Clear Specific Cache

```python
# Clear cache for one pipeline
manager.clear_pipeline_cache('my_narrative')
```

### Clear Old Caches

```python
# Clear caches older than 30 days
manager.clear_old_caches(days=30)
```

### Clear All Caches

```python
# WARNING: Deletes ALL cached data
manager.clear_all_caches(confirm=True)
```

### Check if Cache Exists

```python
if manager.cache_exists('my_narrative'):
    print("Cache found - run will be fast!")
else:
    print("No cache - first run will compute")
```

## Advanced Configuration

### Custom Cache Directory

```python
pipeline = NarrativePipeline(
    'my_narrative',
    cache_dir='/path/to/my/cache'
)
```

### Cache Compression

Joblib automatically compresses cached data. Compression is **enabled by default** for space efficiency.

### Multiple Pipelines

Each pipeline gets its own cache namespace based on `narrative_name`:

```python
# These use separate caches
pipeline1 = NarrativePipeline('linguistic_analysis', ...)
pipeline2 = NarrativePipeline('statistical_baseline', ...)
```

## Performance Examples

### Example 1: IMDB Movie Analysis

```python
# Without caching
pipeline = NarrativePipeline('imdb_analysis', use_cache=False)
# ... add steps ...
pipe = pipeline.build()

# First run: 342 seconds
# Second run: 341 seconds (no caching)
```

```python
# With caching (default)
pipeline = NarrativePipeline('imdb_analysis')
# ... add steps ...
pipe = pipeline.build()

# First run: 342 seconds (computing)
# Second run: 3.2 seconds (cached) - 100x faster!
```

### Example 2: Development Workflow

During development, you often re-run pipelines while tweaking final steps:

```python
pipeline = NarrativePipeline('development', cache_verbose=1)

# Expensive early steps
pipeline.add_step('embeddings', SemanticEmbeddings(), 'Slow embedding generation')
pipeline.add_step('nominative', NominativeAnalysis(), 'Name analysis')

# Quick final step (what you're tweaking)
pipeline.add_step('classifier', LogisticRegression(), 'Final model')

pipe = pipeline.build()

# Run 1: Computes embeddings (slow)
# Run 2: Uses cached embeddings, recomputes only classifier (fast)
# Run 3: After tweaking classifier params, uses cached embeddings again (fast)
```

### Example 3: Multiple Data Splits

```python
pipeline = NarrativePipeline('cross_validation')
# ... build pipeline ...

# Each data split gets its own cache
for fold_idx, (X_train, X_test) in enumerate(cv_splits):
    pipe.fit(X_train, y_train)  # Cached per fold
    X_test_transformed = pipe.transform(X_test)  # Cached per fold
```

## Best Practices

### ‚úÖ DO

- **Use descriptive pipeline names** - They become cache namespaces
- **Enable cache_verbose during development** - See what's happening
- **Clear old caches periodically** - Use `clear_old_caches()`
- **Keep caching enabled** - Default behavior, maximum benefit

### ‚ùå DON'T

- **Don't use identical names** - Different pipelines need unique names
- **Don't manually edit cache** - Use CacheManager instead
- **Don't assume cache works across machines** - Paths differ

### When to Disable Caching

Disable caching (`use_cache=False`) when:
- Running quick one-off experiments
- Testing if caching is causing issues
- Profiling pure compute time
- Working with tiny datasets (< 100 samples)

## Troubleshooting

### Cache Not Being Used

If second run is still slow:

```python
# Enable verbose mode to diagnose
pipeline = NarrativePipeline('my_narrative', cache_verbose=10)
```

Check for:
- Different random seeds in transformers
- Non-deterministic operations
- Timestamp-based features

### Cache Size Too Large

```python
# Check what's taking space
manager = get_cache_manager()
manager.print_cache_report()

# Clear large caches
manager.clear_pipeline_cache('large_pipeline')
```

### Cache Location

Default location: `~/.narrative_cache/pipelines/`

To change:
```python
pipeline = NarrativePipeline(
    'my_narrative',
    cache_dir='/mnt/fast_disk/cache'
)
```

### Permission Errors

If cache directory isn't writable, caching fails silently. Check:
```bash
ls -la ~/.narrative_cache/
```

## Technical Details

### Implementation

- Uses **joblib.Memory** for caching
- Integrates with **sklearn.pipeline.Pipeline**
- Stores cached data as pickled files
- Hashes input data for cache keys
- Tracks code dependencies for invalidation

### Cache Storage Format

```
~/.narrative_cache/
‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îú‚îÄ‚îÄ my_narrative/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ joblib/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sklearn.pipeline/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _fit_transform_one/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ abc123def.pkl  # Cached results
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata.json
```

### Memory Overhead

Caching requires disk space proportional to:
- Number of pipeline steps
- Size of transformed data
- Number of unique datasets

Typical sizes:
- Small pipeline (3 steps, 1000 samples): ~10 MB
- Large pipeline (10 steps, 100K samples): ~500 MB

### Thread Safety

Joblib Memory is **thread-safe** with file locking. Multiple processes can safely share the same cache.

## API Reference

### NarrativePipeline

```python
NarrativePipeline(
    narrative_name: str,           # Cache namespace
    use_cache: bool = True,        # Enable caching
    cache_dir: str = None,         # Custom location
    cache_verbose: int = 0,        # Verbosity (0-10)
    hypothesis: str = "",
    expected_outcome: str = "",
    domain_assumptions: List[str] = None
)
```

### CacheManager

```python
manager = get_cache_manager()

# Inspection
stats = manager.get_cache_stats()
manager.print_cache_report()
manager.cache_exists(pipeline_name)

# Cleanup
manager.clear_pipeline_cache(pipeline_name)
manager.clear_old_caches(days=30)
manager.clear_all_caches(confirm=True)
```

### CachedPipeline (Low-Level)

For direct sklearn Pipeline wrapping:

```python
from narrative_optimization.src.pipelines import make_cached_pipeline

pipe = Pipeline([...])
cached_pipe = make_cached_pipeline(pipe, verbose=1)
```

## Examples

See `narrative_optimization/examples/caching_demo.py` for comprehensive examples:

```bash
python narrative_optimization/examples/caching_demo.py
```

Demonstrates:
- Basic caching benefits
- Cache management
- Automatic invalidation
- Multiple pipelines

## Summary

**Caching is now enabled by default** and provides massive speedups with zero configuration required. The system is intelligent enough to automatically invalidate when needed, ensuring correctness while maximizing performance.

For most use cases, you don't need to think about caching - it just works. For advanced usage, the CacheManager provides full control over cache lifecycle and monitoring.

**Happy caching! ‚ö°**

