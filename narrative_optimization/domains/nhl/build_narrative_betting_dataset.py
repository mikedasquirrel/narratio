#!/usr/bin/env python3
"""
NHL Narrative Betting Dataset Builder

Combines:
1. Structured NHL features (performance + nominative) from the legacy extractor
2. Cached narrative transformer features (744 cols) generated by the universal processor
3. Metadata about each game (teams, date, outcome)

Enhancements:
- Optional integration of Absolute Maximum snapshot odds to replace legacy
  estimated prices with bookmaker-derived lines.
- CLI configuration for custom input/output locations.

Output:
  - Parquet file with all features for downstream model training
  - NPZ/JSON summaries for compatibility with existing tooling
"""

import argparse
import json
import pickle
import re
import unicodedata
from datetime import datetime, date
from pathlib import Path
from typing import Dict, Optional, Tuple, List

import numpy as np
import pandas as pd
from pandas.api import types as ptypes


PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
DATA_PATH = PROJECT_ROOT / "data" / "domains" / "nhl_games_with_odds.json"
STRUCTURED_PATH = (
    PROJECT_ROOT / "narrative_optimization" / "domains" / "nhl" / "nhl_features_complete.npz"
)
OUTPUT_DIR = PROJECT_ROOT / "narrative_optimization" / "domains" / "nhl"
ANALYSIS_DIR = PROJECT_ROOT / "analysis"
DEFAULT_ABSMAX_PARQUET = ANALYSIS_DIR / "nhl_absolute_max_snapshot.parquet"
CACHE_DIR = PROJECT_ROOT / "narrative_optimization" / "cache" / "features"

TEAM_NAME_MAP = {
    "ANA": "Anaheim Ducks",
    "ARI": "Arizona Coyotes",
    "ATL": "Atlanta Thrashers",
    "BOS": "Boston Bruins",
    "BUF": "Buffalo Sabres",
    "CAR": "Carolina Hurricanes",
    "CBJ": "Columbus Blue Jackets",
    "CGY": "Calgary Flames",
    "CHI": "Chicago Blackhawks",
    "COL": "Colorado Avalanche",
    "DAL": "Dallas Stars",
    "DET": "Detroit Red Wings",
    "EDM": "Edmonton Oilers",
    "FLA": "Florida Panthers",
    "LAK": "Los Angeles Kings",
    "MIN": "Minnesota Wild",
    "MTL": "Montreal Canadiens",
    "NJD": "New Jersey Devils",
    "NSH": "Nashville Predators",
    "NYI": "New York Islanders",
    "NYR": "New York Rangers",
    "OTT": "Ottawa Senators",
    "PHI": "Philadelphia Flyers",
    "PHX": "Phoenix Coyotes",
    "PIT": "Pittsburgh Penguins",
    "SEA": "Seattle Kraken",
    "SJS": "San Jose Sharks",
    "STL": "St Louis Blues",
    "TBL": "Tampa Bay Lightning",
    "TOR": "Toronto Maple Leafs",
    "VAN": "Vancouver Canucks",
    "VGK": "Vegas Golden Knights",
    "WPG": "Winnipeg Jets",
    "WSH": "Washington Capitals",
}


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Build the NHL narrative betting dataset with optional Absolute Max odds.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument(
        "--games-path",
        type=Path,
        default=DATA_PATH,
        help="Path to the base NHL games JSON file.",
    )
    parser.add_argument(
        "--structured-path",
        type=Path,
        default=STRUCTURED_PATH,
        help="Path to the structured features NPZ artifact.",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=OUTPUT_DIR,
        help="Destination directory for dataset artifacts.",
    )
    parser.add_argument(
        "--absolute-max-parquet",
        type=Path,
        default=DEFAULT_ABSMAX_PARQUET,
        help="Absolute Maximum snapshot Parquet file.",
    )
    parser.add_argument(
        "--use-absolute-max-odds",
        action="store_true",
        help="Replace legacy betting_odds with Absolute Maximum bookmaker lines when available.",
    )
    return parser.parse_args()


def load_games(source_path: Path) -> Tuple[pd.DataFrame, Dict]:
    with open(source_path, "r") as f:
        games = json.load(f)
    df = pd.DataFrame(games)
    return df, games


def load_structured_features(game_ids, structured_path: Path = STRUCTURED_PATH) -> pd.DataFrame:
    if not structured_path.exists():
        raise FileNotFoundError(f"Structured feature file missing: {structured_path}")
    data = np.load(structured_path)
    structured = data["features"]
    stored_ids = list(map(str, data["game_ids"]))
    if list(map(str, game_ids)) != stored_ids:
        raise ValueError("Game ID order mismatch between structured features and source data")

    def _find_metadata_path(base_path: Path) -> Optional[Path]:
        candidates = [
            base_path.with_suffix(".json"),
            base_path.parent / f"{base_path.stem}_metadata.json",
            base_path.parent / "nhl_features_metadata.json",
        ]
        for cand in candidates:
            if cand.exists():
                return cand
        return None

    columns: Optional[List[str]] = None
    metadata_path = _find_metadata_path(structured_path)
    if metadata_path:
        try:
            with metadata_path.open("r") as f:
                metadata = json.load(f)
            feature_breakdown = metadata.get("feature_breakdown", {})
            universal_count = feature_breakdown.get("universal")
            perf_count = feature_breakdown.get("performance")
            nom_count = feature_breakdown.get("nominative")

            universal_names = metadata.get("universal", {}).get("feature_names")
            if universal_names and isinstance(universal_names, list):
                columns = list(universal_names)
            elif universal_count:
                columns = [f"universal_{i}" for i in range(universal_count)]
            else:
                columns = []

            if perf_count:
                perf_names = metadata.get("performance", {}).get("feature_names")
                if perf_names and isinstance(perf_names, list):
                    columns.extend(perf_names)
                else:
                    start = len(columns)
                    columns.extend([f"performance_{i-start}" for i in range(start, start + perf_count)])

            if nom_count:
                nom_names = metadata.get("nominative", {}).get("feature_names")
                if nom_names and isinstance(nom_names, list):
                    columns.extend(nom_names)
                else:
                    start = len(columns)
                    columns.extend([f"nominative_{i-start}" for i in range(start, start + nom_count)])
        except Exception as exc:  # pragma: no cover - metadata issues fallback
            print(f"âš ï¸  Failed to read structured metadata ({exc}); falling back to generic column names.")
            columns = None

    if not columns or len(columns) != structured.shape[1]:
        columns = [f"feature_{i}" for i in range(structured.shape[1])]

    return pd.DataFrame(structured, columns=columns)


def _expand_numeric_dict_column(series: pd.Series, prefix: str) -> pd.DataFrame:
    """Normalize a column of dicts into numeric dataframe with prefix."""
    if series is None or series.empty:
        return pd.DataFrame()
    records = []
    for item in series:
        records.append(item if isinstance(item, dict) else {})
    if not records:
        return pd.DataFrame()
    normalized = pd.json_normalize(records)
    if normalized.empty:
        return normalized
    numeric_cols = normalized.select_dtypes(include=["number", "bool"]).columns
    normalized = normalized[numeric_cols].copy()
    for col in normalized.columns:
        if normalized[col].dtype == bool:
            normalized[col] = normalized[col].astype(int)
    normalized = normalized.apply(pd.to_numeric, errors="coerce")
    normalized = normalized.add_prefix(prefix)
    return normalized


def _add_derived_betting_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if {"odds_implied_prob_home", "odds_implied_prob_away"}.issubset(df.columns):
        df["odds_implied_edge"] = (
            df["odds_implied_prob_home"] - df["odds_implied_prob_away"]
        )
        df["odds_market_efficiency"] = (
            df["odds_implied_prob_home"] + df["odds_implied_prob_away"]
        )
    if {"odds_moneyline_home", "odds_moneyline_away"}.issubset(df.columns):
        df["odds_moneyline_gap"] = (
            df["odds_moneyline_home"] - df["odds_moneyline_away"]
        )
    if {"odds_total", "odds_over_odds", "odds_under_odds"}.issubset(df.columns):
        df["odds_total_pressure"] = df["odds_over_odds"] - df["odds_under_odds"]
    return df


def _add_derived_context_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if {"ctx_home_rest_days", "ctx_away_rest_days"}.issubset(df.columns):
        df["ctx_rest_gap"] = df["ctx_home_rest_days"] - df["ctx_away_rest_days"]
    if {"ctx_home_win_pct", "ctx_away_win_pct"}.issubset(df.columns):
        df["ctx_win_pct_gap"] = df["ctx_home_win_pct"] - df["ctx_away_win_pct"]
    if "ctx_rest_advantage" not in df.columns and {"ctx_home_rest_days", "ctx_away_rest_days"}.issubset(df.columns):
        df["ctx_rest_advantage"] = df["ctx_rest_gap"]
    return df


def _sanitize_numeric_block(df: pd.DataFrame, skip: Optional[set] = None) -> pd.DataFrame:
    """Fill NaN/inf values column-wise using medians or modes."""
    if df is None or df.empty:
        return df
    skip = skip or set()
    result = df.copy()
    for col in result.columns:
        if col in skip:
            continue
        series = result[col]
        if ptypes.is_bool_dtype(series):
            if series.isnull().any():
                mode = series.mode(dropna=True)
                fill_value = bool(mode.iloc[0]) if not mode.empty else False
                result[col] = series.fillna(fill_value)
        elif ptypes.is_numeric_dtype(series):
            series = series.replace([np.inf, -np.inf], np.nan)
            if series.isnull().all():
                result[col] = series.fillna(0.0)
            else:
                median = series.median()
                if pd.isna(median):
                    median = 0.0
                result[col] = series.fillna(median)
    return result


def _resolve_cache_key(game_count: int) -> Tuple[str, Dict]:
    """Locate the freshest NHL cache artifact that matches the dataset."""
    candidates = sorted(CACHE_DIR.glob("nhl_*_metadata.json"),
                        key=lambda p: p.stat().st_mtime,
                        reverse=True)
    for metadata_path in candidates:
        try:
            with open(metadata_path, "r") as f:
                metadata = json.load(f)
        except Exception:
            continue
        if metadata.get("domain") != "nhl":
            continue
        if metadata.get("n_narratives") != game_count:
            continue
        cache_key = metadata_path.stem.replace("_metadata", "")
        features_path = CACHE_DIR / f"{cache_key}.pkl"
        if features_path.exists():
            return cache_key, metadata
    raise FileNotFoundError(
        f"No cached NHL features matched {game_count} games. "
        "Run the universal_domain_processor for 'nhl' to refresh the cache."
    )


def load_narrative_features(game_count: int) -> pd.DataFrame:
    cache_key, metadata = _resolve_cache_key(game_count)
    features_path = CACHE_DIR / f"{cache_key}.pkl"
    with open(features_path, "rb") as f:
        features = pickle.load(f)
    if features.shape[0] != game_count:
        raise ValueError(f"Narrative features rows {features.shape[0]} != games {game_count}")
    columns = metadata.get("feature_names")
    if not columns or len(columns) != features.shape[1]:
        columns = [f"narr_{i}" for i in range(features.shape[1])]
    print(f"ðŸŒ Using cached narrative features: {cache_key}")
    return pd.DataFrame(features, columns=columns), cache_key, metadata


def normalize_team_name(name: Optional[str]) -> str:
    if not name:
        return ""
    text = unicodedata.normalize("NFKD", str(name))
    text = "".join(ch for ch in text if not unicodedata.combining(ch))
    text = text.lower().replace("&", " and ")
    text = re.sub(r"[^a-z0-9]+", "_", text)
    text = re.sub(r"_+", "_", text)
    return text.strip("_")


def resolve_full_team_name(abbrev: Optional[str]) -> Optional[str]:
    if not abbrev:
        return None
    return TEAM_NAME_MAP.get(str(abbrev).upper(), abbrev)


def build_match_key(game_date: Optional[str], home_name: Optional[str], away_name: Optional[str]) -> Optional[str]:
    if not game_date or not home_name or not away_name:
        return None
    if isinstance(game_date, (datetime, date)):
        date_str = game_date.strftime("%Y-%m-%d")
    else:
        date_str = str(game_date)
    return f"{date_str}_{normalize_team_name(home_name)}_{normalize_team_name(away_name)}"


def safe_float(value: Optional[float]) -> Optional[float]:
    try:
        if value is None or (isinstance(value, float) and np.isnan(value)):
            return None
        return float(value)
    except (TypeError, ValueError):
        return None


def american_to_probability(odds: Optional[float]) -> Optional[float]:
    value = safe_float(odds)
    if value is None or value == 0:
        return None
    if value < 0:
        return abs(value) / (abs(value) + 100)
    return 100 / (value + 100)


def load_absolute_max_odds(parquet_path: Path) -> pd.DataFrame:
    if not parquet_path.exists():
        raise FileNotFoundError(
            f"Absolute Maximum snapshot Parquet not found: {parquet_path}. "
            "Run analysis/nhl_absolute_max_snapshot.py first."
        )
    df = pd.read_parquet(parquet_path)
    if df.empty:
        raise ValueError(f"Absolute Maximum Parquet is empty: {parquet_path}")
    commence = pd.to_datetime(df["commence_time"], utc=True, errors="coerce")
    df["match_date"] = commence.dt.date.astype(str)
    df["norm_home_name"] = df["home_team"].apply(normalize_team_name)
    df["norm_away_name"] = df["away_team"].apply(normalize_team_name)
    df["match_key"] = df["match_date"] + "_" + df["norm_home_name"] + "_" + df["norm_away_name"]
    df["last_market_update_dt"] = pd.to_datetime(df["last_market_update"], utc=True, errors="coerce")
    df = (
        df.sort_values("last_market_update_dt")
        .drop_duplicates(subset="match_key", keep="last")
        .dropna(subset=["match_key"])
    )
    return df


def build_betting_dict_from_snapshot(snapshot_row: Dict) -> Dict:
    moneyline_home = safe_float(snapshot_row.get("moneyline_home_avg"))
    moneyline_away = safe_float(snapshot_row.get("moneyline_away_avg"))
    puck_line_home = safe_float(snapshot_row.get("spread_home_points_avg"))
    puck_line_away = safe_float(snapshot_row.get("spread_away_points_avg"))
    puck_line_home_odds = safe_float(snapshot_row.get("spread_home_price_avg"))
    puck_line_away_odds = safe_float(snapshot_row.get("spread_away_price_avg"))

    betting_dict = {
        "moneyline_home": moneyline_home,
        "moneyline_away": moneyline_away,
        "puck_line_home": puck_line_home,
        "puck_line_away": puck_line_away,
        "puck_line_odds": puck_line_home_odds,
        "puck_line_home_odds": puck_line_home_odds,
        "puck_line_away_odds": puck_line_away_odds,
        "total": safe_float(snapshot_row.get("total_points_avg")),
        "over_odds": safe_float(snapshot_row.get("total_over_price_avg")),
        "under_odds": safe_float(snapshot_row.get("total_under_price_avg")),
        "implied_prob_home": american_to_probability(moneyline_home),
        "implied_prob_away": american_to_probability(moneyline_away),
        "source": "absolute_max_snapshot",
        "note": "Bookmaker consensus derived from Absolute Maximum snapshot",
        "snapshot_event_id": snapshot_row.get("event_id"),
        "snapshot_commence_time": snapshot_row.get("commence_time"),
        "bookmaker_count": int(snapshot_row.get("bookmaker_count") or 0),
        "bookmakers_observed": snapshot_row.get("bookmaker_keys"),
        "markets_observed": snapshot_row.get("market_keys"),
        "last_market_update": snapshot_row.get("last_market_update"),
    }
    return betting_dict


def apply_absolute_max_odds(df_games: pd.DataFrame, parquet_path: Path) -> Tuple[pd.DataFrame, Dict]:
    odds_df = load_absolute_max_odds(parquet_path)
    lookup = odds_df.set_index("match_key").to_dict(orient="index")

    enriched = df_games.copy()
    enriched["home_full_name"] = enriched["home_team"].apply(resolve_full_team_name)
    enriched["away_full_name"] = enriched["away_team"].apply(resolve_full_team_name)
    enriched["match_key"] = enriched.apply(
        lambda row: build_match_key(row.get("date"), row["home_full_name"], row["away_full_name"]),
        axis=1,
    )

    matched = 0
    updated_odds = []
    missing_keys = []

    for _, row in enriched.iterrows():
        key = row.get("match_key")
        snapshot_row = lookup.get(key) if key else None
        if snapshot_row:
            matched += 1
            updated_odds.append(build_betting_dict_from_snapshot(snapshot_row))
        else:
            missing_keys.append(key)
            current_odds = row.get("betting_odds")
            updated_odds.append(current_odds)

    enriched["betting_odds"] = updated_odds
    coverage = matched / len(enriched) if len(enriched) else 0
    stats = {
        "matches": matched,
        "total_games": len(enriched),
        "coverage_pct": round(coverage * 100, 2),
        "unmatched_examples": [key for key in missing_keys[:5] if key],
        "snapshot_path": str(parquet_path),
    }

    enriched.drop(columns=["home_full_name", "away_full_name", "match_key"], inplace=True)
    return enriched, stats


def main():
    args = parse_args()
    output_dir = args.output_dir.expanduser().resolve()
    output_dir.mkdir(parents=True, exist_ok=True)

    games_path = args.games_path.expanduser().resolve()
    print(f"ðŸ“‚ Loading NHL games from {games_path}")
    df_games, games = load_games(games_path)

    absmax_stats = None
    if args.use_absolute_max_odds:
        parquet_path = args.absolute_max_parquet.expanduser().resolve()
        print(f"ðŸŽ¯ Infusing Absolute Maximum odds from {parquet_path}")
        df_games, absmax_stats = apply_absolute_max_odds(df_games, parquet_path)
    game_ids = df_games["game_id"].tolist()
    outcome = df_games.get("home_won", df_games.get("winner"))
    base_df = pd.DataFrame({
        "game_id": game_ids,
        "season": df_games.get("season"),
        "date": df_games.get("date"),
        "home_team": df_games.get("home_team"),
        "away_team": df_games.get("away_team"),
        "home_won": outcome
    })

    print("ðŸ“Š Loading structured NHL features...")
    structured_df = load_structured_features(game_ids, args.structured_path.expanduser().resolve())

    structured_has_universal = any(
        col.startswith("StatisticalTransformer_") or col.startswith("universal_")
        for col in structured_df.columns
    )

    if structured_has_universal:
        print("ðŸŒ Universal narrative features already embedded in structured dataset; skipping external narrative cache.")
        narrative_df = pd.DataFrame(index=structured_df.index)
        cache_key = "embedded_in_structured"
        narrative_meta = {"timestamp": None}
    else:
        print("ðŸŒ Loading narrative transformer features...")
        narrative_df, cache_key, narrative_meta = load_narrative_features(len(df_games))

    print("ðŸ“ˆ Expanding betting + context signals...")
    odds_df = _expand_numeric_dict_column(df_games.get("betting_odds"), "odds_")
    if not odds_df.empty:
        odds_df = _add_derived_betting_features(odds_df)
        odds_df = _sanitize_numeric_block(odds_df)
    context_df = _expand_numeric_dict_column(df_games.get("temporal_context"), "ctx_")
    if not context_df.empty:
        context_df = _add_derived_context_features(context_df)
        context_df = _sanitize_numeric_block(context_df)

    print("ðŸ”— Combining features...")
    frames = [
        base_df.reset_index(drop=True),
        structured_df.reset_index(drop=True),
        narrative_df.reset_index(drop=True)
    ]
    if not odds_df.empty:
        frames.append(odds_df.reset_index(drop=True))
    if not context_df.empty:
        frames.append(context_df.reset_index(drop=True))
    combined = pd.concat(frames, axis=1)
    combined = _sanitize_numeric_block(combined, skip={"home_won"})

    parquet_path = output_dir / "nhl_narrative_betting_dataset.parquet"
    npz_path = output_dir / "nhl_narrative_betting_dataset.npz"
    metadata_path = output_dir / "nhl_narrative_betting_metadata.json"

    print(f"ðŸ’¾ Saving Parquet -> {parquet_path}")
    combined.to_parquet(parquet_path, index=False)

    print(f"ðŸ’¾ Saving NPZ -> {npz_path}")
    np.savez_compressed(
        npz_path,
        game_id=combined["game_id"].values,
        features=combined.drop(columns=["game_id"]).values,
        columns=[col for col in combined.columns if col != "game_id"]
    )

    metadata = {
        "n_games": len(combined),
        "total_features": combined.shape[1] - 1,
        "structured_features": structured_df.shape[1],
        "narrative_features": narrative_df.shape[1],
        "betting_feature_columns": odds_df.columns.tolist() if not odds_df.empty else [],
        "context_feature_columns": context_df.columns.tolist() if not context_df.empty else [],
        "narrative_cache_key": cache_key,
        "narrative_cache_timestamp": narrative_meta.get("timestamp"),
        "columns": combined.columns.tolist(),
    }
    if absmax_stats:
        metadata["absolute_max_odds"] = absmax_stats
    print(f"ðŸ’¾ Saving metadata -> {metadata_path}")
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=2)

    print("\nâœ… NHL narrative betting dataset built successfully")


if __name__ == "__main__":
    main()

